\documentclass{article}

\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{bbm}

\begin{document}

\tableofcontents

\pagebreak

\section{Introduction to Machine Learning}

\begin{itemize}
    \item Machine learning algorithms are statistical algorithms that can learn from data and generalize to unseen data, thus performing tasks without explicit instructions.
    \item In supervised methods we know the outcome Y for each input X; often, the task is to predict Y based on X.
    \item In unsupervised methods we have only the inputs X. Typical tasks are about finding structure in data.
    \begin{itemize}
        \item For example, it can be used to find "clusters".
    \end{itemize}
    \item X is often known as:
    \begin{itemize}
        \item Input
        \item Feature
        \item Predictor
        \item Covariate
        \item Independent variable
    \end{itemize}
    \item Y is often known as:
    \begin{itemize}
        \item Output
        \item Outcome
        \item Response
        \item Target variable
        \item Dependent variable
    \end{itemize}
    \item Supervised problems are often referred to as either:
    \begin{itemize}
        \item \textbf{Regression}: The outcome Y is quantitative (typically $\mathbb{R}$)s
        \item \textbf{Classification}: The outcome Y is categorical
    \end{itemize}
\end{itemize}

\pagebreak

\section{Linear Regression}

\subsection{Simple Linear Regression}
Simple linear regression models the relationship between a single predictor \(X\) and a response \(Y\) using a linear function:
\[
Y = \beta_0 + \beta_1 X + \epsilon,
\]
where:
\begin{itemize}
    \item \(\beta_0\) is the intercept,
    \item \(\beta_1\) is the slope of the line,
    \item \(\epsilon\) is the error term, which is assumed to be normally distributed with mean 0.
\end{itemize}
The coefficients \(\beta_0\) and \(\beta_1\) are estimated using the least squares criterion, which minimizes the residual sum of squares (RSS):
\[
RSS = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 = \sum_{i=1}^{n} (y_i - (\beta_0 + \beta_1 x_i))^2.
\]

\textbf{Interpretation:} 
\begin{itemize}
    \item \(\beta_1\) represents the average change in the response \(Y\) associated with a one-unit change in the predictor \(X\).
    \item For example, in predicting sales based on TV advertising spend, \(\beta_1\) would indicate how much sales increase, on average, for each additional unit of advertising expenditure on TV.
\end{itemize}

\subsection{Multiple Linear Regression}
Multiple linear regression extends the simple linear regression model to include multiple predictors:
\[
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_p X_p + \epsilon.
\]
Here:
\begin{itemize}
    \item Each \(\beta_j\) represents the change in \(Y\) associated with a one-unit change in \(X_j\), holding all other predictors constant.
    \item The model can be written in matrix notation as:
    \[
    Y = X\beta + \epsilon,
    \]
    where \(Y\) is an \(n \times 1\) vector of responses, \(X\) is an \(n \times (p+1)\) matrix of predictors (including a column of ones for the intercept), \(\beta\) is a \((p+1) \times 1\) vector of coefficients, and \(\epsilon\) is an \(n \times 1\) vector of errors.
\end{itemize}

\textbf{Example: Advertising Data.} Consider a multiple regression model for predicting sales using TV, radio, and newspaper advertising budgets:
\[
\text{sales} = \beta_0 + \beta_1 \cdot \text{TV} + \beta_2 \cdot \text{radio} + \beta_3 \cdot \text{newspaper} + \epsilon.
\]
The estimated coefficients \(\beta_1, \beta_2,\) and \(\beta_3\) help to understand the individual contribution of each type of advertising while accounting for the other types.

\subsection{3.3 Other Considerations in the Regression Model}
\textbf{Qualitative Predictors:} 
Qualitative or categorical predictors can be incorporated into regression models by creating dummy variables. For example, if the predictor is a categorical variable 'region' with three levels (East, West, North), we can introduce two dummy variables:
\[
\text{region}_1 = 
\begin{cases} 
1 & \text{if East} \\
0 & \text{otherwise}
\end{cases}, \quad 
\text{region}_2 = 
\begin{cases} 
1 & \text{if West} \\
0 & \text{otherwise}
\end{cases}.
\]
Incorporating these into the regression model allows us to estimate different intercepts for each region.

\textbf{Interaction Terms:}
Interaction terms allow modeling situations where the effect of one predictor depends on another. For example, if we believe that the effect of TV advertising on sales depends on the level of radio advertising, we can include an interaction term:
\[
\text{sales} = \beta_0 + \beta_1 \cdot \text{TV} + \beta_2 \cdot \text{radio} + \beta_3 \cdot (\text{TV} \times \text{radio}) + \epsilon.
\]

\textbf{Extensions and Potential Problems:}
\begin{itemize}
    \item \textbf{Polynomial Regression:} Adds polynomial terms to the regression model to capture non-linear relationships.
    \item \textbf{Potential Issues:} Multicollinearity (high correlation among predictors), non-linearity of relationships, and outliers can affect model estimates.
\end{itemize}


\section{Bias and Variance}

Assuming $Y=f(X_0)+\varepsilon$, where:
\begin{itemize}
    \item $\mathbb{E}(\varepsilon)=0$
    \item $\varepsilon$ is independent of $X_0$
\end{itemize}
When fitting $\hat{f}$ with a dataset of $N$ observations ($X$,$Y$)
\begin{itemize}
    \item $N$ observations = \textit{training data}
    \item Algorithm for fitting $\hat{f}$ = \textit{learner}
    \item Applying learner to the training data = \textit{training}
\end{itemize}
Given training data and a chosen model framework, we want to minimize \textit{expected test MSE}: 
\begin{align*}
    \mathbb{E}[(Y_0-\hat{f}(X_0))^2]
\end{align*}
The expected test MSE at $x_0$ is:
\begin{align*}
    \mathbb{E}(Y_0-\hat{f}(x_0))=\mathbb{E}(\hat{f}(x_0)-\mathbb{E}(\hat{f}(x_0)))^2+(\mathbb{E}(\hat{f}(x_0))-f(x_0))^2+\text{Var}(\varepsilon)
\end{align*}
Where:
\begin{itemize}
    \item $\mathbb{E}(\hat{f}(x_0)-\mathbb{E}(\hat{f}(x_0)))^2$ is the Variance of $\hat{f}(x_0)$, the amount by which $\hat{f}$ would change if it were estimated with a different training data set
    \item $\mathbb{E}(\hat{f}(x_0))-f(x_0)$ is the Bias of $\hat{f}(X_0)$, the expected deviation of the model prediction from the true value - a.k.a., the inability to capture the true relationship
\end{itemize}
In the MSE, all 3 terms are non-negative, so any large term = large MSE; however, variance and bias are both \textit{reducible errors}.

\subsection{Validation sets}

If we wish to use MSE for model building - selecting features and tuning hyperparameters - we create a validation set, separate from training/testing set, for estimating the MSE.

\subsubsection{Leave-one-out crossvalidation (LOOCV)}

Create $n$ different partitions (where $n$ is amount of data points), with just one validation data point in each.
For each partition, $i$, the test MSE is $(y_i-\hat{y_i})^2$.
The LOOCV estimate for the test MSE is:
\begin{align*}
    \text{MSE} = \frac{1}{n}\sum_{i=n}^{n}(y_i-\hat{y_i})^2
\end{align*}
Downside: This is computationally expensive, requiring $n$ fits.

\subsubsection{$k$-fold crossvalidation}

Create $k$ different partitions ("folds"), in each $1/k$ of the data is for validation. Randomly shuffle data beforehand if it's ordered.

For each fold, compute $\text{MSE}_i$. Then, average:
\begin{align*}
    \frac{1}{k}\sum_{i=1}^{k}\text{MSE}_k
\end{align*}
Downside: Non-deterministic.

\section{Regression with many features}

Problem:
\begin{itemize}
    \item When features $p \approx n$, not enough data to estimate the regression curve well.
    \item When $p > n$, MLE doesn't exist.
\end{itemize}

Solutions:
\begin{itemize}
    \item Model selection
    \item Shrinkage (regularisation)
\end{itemize}

\subsection{Model selection}

As mentioned in 2.6.2, several methods:
\begin{itemize}
    \item \textbf{Best subset:} Self-explanitory, expensive
    \begin{itemize}
        \item Genetic algorithm can be used as heuristic
    \end{itemize}
    \item \textbf{Forward selection:} Start by including no/few variables
    \item \textbf{Backward selection:} Start by including all/many variables
    \item \textbf{Alternating:} Alternate between forward and backward selection
\end{itemize}

For comparing models, AIC, BIC, or test error can be used.

\subsubsection{AIC and BIC}

\begin{align*}
    \text{AIC}=-2\log\hat{L}+2p
\end{align*}
\begin{align*}
    \text{BIC}=-2\log\hat{L}+p \log n
\end{align*}
Where $p$ is the number of parameters, $n$ is the number of features, and d $\hat{L}$ is the maximised likelihood function. Smaller = better.

\subsubsection{Stepwise selection}

\textbf{Backwards:}
\begin{enumerate}
    \item Make initial model, with all/many features
    \item Iterative remove the least relevant feature (largest drop in AIC)
    \item Stop if removing a feature does not drop AIC significantly
\end{enumerate}

\textbf{Forwards:}
\begin{enumerate}
    \item Make initial model, with only intercept/few features
    \item Iterative add the most relevant feature (largest drop in AIC)
    \item Stop if adding a feature does not drop AIC significantly
\end{enumerate}

\subsubsection{Notes on automatic model selection}

\begin{itemize}
    \item One may wish to keep certain variables, even if they are not significant, if they are relevant in some other way, such as to the research question.
    \item One may wish to remove certain variables, even if they are significant, if they complicate interpretations.
\end{itemize}

\subsection{Shrinkage}

Shrinkage helps estimate coefficients $\beta$ in a constrained way, keeping them small. Doing so reduces coefficient variance and can improve the fit.

\subsubsection{Ridge regression}

Ridge regression minimizes the residual sum of squares, but adds a penalty term proportional to the squared magnitude of the coefficients:

\begin{align*}
    \hat{\beta}_\text{ridge} = \underset{\beta}{\mathrm{argmin}} \left\{ \sum_{i=1}^{n} (y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij})^2 + \lambda \sum_{j=1}^{p} \beta_j^2 \right\}
\end{align*}

Where:
\begin{itemize}
    \item $\lambda \geq 0$ is a tuning parameter that controls the strength of the penalty.
    \item A larger $\lambda$ leads to more shrinkage and smaller coefficients.
    \item Ridge regression shrinks coefficients towards zero but never sets them exactly to zero.
    \item $\text{argmin}$ (short for "argument of the minimum") finds the value of $\beta$ that minimizes the function inside the curly braces. In this case, it finds the coefficients that minimize the sum of squared errors plus the penalty term.
\end{itemize}

\subsubsection{Lasso}

Lasso (Least Absolute Shrinkage and Selection Operator) regression also minimizes the residual sum of squares, but it adds a penalty term proportional to the absolute value of the coefficients:

\begin{align*}
    \hat{\beta}^\text{lasso} = \underset{\beta}{\mathrm{argmin}} \left\{ \sum_{i=1}^{n} (y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij})^2 + \lambda \sum_{j=1}^{p} |\beta_j| \right\}
\end{align*}

Where:
\begin{itemize}
    \item $\lambda \geq 0$ is a tuning parameter that controls the strength of the penalty.
    \item Lasso encourages sparsity, meaning that some coefficients are set exactly to zero for sufficiently large $\lambda$.
    \item Lasso can be used for both regularization and feature selection.
\end{itemize}

\subsubsection{Lasso vs Ridge}

\textbf{TLDR}: Ridge scales everything down, while Lasso also gets rid of small enough coefficients.

\begin{itemize}
    \item \textbf{Ridge:}
    \begin{itemize}
        \item Ridge regression applies an $L_2$ penalty, which shrinks the coefficients continuously towards zero.
        \item It does not perform feature selection as it keeps all coefficients, though they may be very small.
    \end{itemize}
    
    \item \textbf{Lasso:}
    \begin{itemize}
        \item Lasso applies an $L_1$ penalty, which shrinks some coefficients to zero exactly.
        \item It performs both regularization and feature selection by setting some coefficients to zero.
    \end{itemize}
    
    \item \textbf{Choosing between them:}
    \begin{itemize}
        \item Lasso is preferred when you suspect many features are irrelevant and need automatic feature selection.
        \item Ridge is preferred when all features are potentially relevant but need regularization to prevent overfitting.
        \item Elastic net is a compromise between Ridge and Lasso, applying a combination of $L_1$ and $L_2$ penalties.
    \end{itemize}
\end{itemize}

\section{Classification with Logistic Regression}

\begin{itemize}
    \item Logistic regression seeks to:
    \begin{itemize}
        \item Model the probability of an event occurring depending on the values of the independent variables, which can be categorical or numerical
        \item Estimate the probability that and vent occurs for a randomly selected observation versus the probability that the event does not occur
        \item Predict the effect of a series of variables on a binary response (dependent) variable
        \item Classify observations by estimating the probability that an observation is in a particular category
    \end{itemize}
    \item Binary data does not have a normal distribution, which is needed for linear regression
    \item For logistic regression, we use the $\chi^2$-squared test instead of $F$-test
\end{itemize}

\subsection{Odds}

\begin{itemize}
    \item $\text{odds}=\frac{P(\text{occurring})}{P(\text{not occurring})}=\frac{p}{1-p}$
    \item $p=\frac{e^{\log(\text{odds})}}{1+e^{\log(\text{odds})}}$
    \item $\text{odds ratio}=\frac{\text{odds}_1}{\text{odds}_0}$ for two events with probabilities $p_0$ and $p_1$
    \item In logistic regression for an independent variable, the odds ratio represents how the odds change with a 1 unit increase in the variable, holding all other variables constant
    \item The odds ratio holds constant for the entire range of the variable
\end{itemize}

\subsection{Logit}

\begin{itemize}
    \item The dependent variable in logistic regression follows the Bernoulli distribution having an unknown probability $p$
    \item In logistic regression we are estimating an unknown $p$ for any given linear combination of independent variables. This estimate is $\hat{p}$
    \item We need to link our independent variables to the Bernoulli distribution; that link is called the logit
    \item Logit - the natural log of the odds ratio - maps linear combinations of variables onto the Bernoulli probability distribution with a domain from 0 to 1
    \item $\text{logit}(p)=\ln(\text{odds})=\ln(\frac{p}{1-p})=\ln(p)-\ln(1-p)$
    \item $\text{logit}$ is undefined at $p=0$ and $p=1$
    \item The logit function runs from 0 to 1 along the x-axis, not the y-axis. This we use inverse logit
    \item $\text{logit}^{-1}(\alpha)=\mu_{y|x}=\frac{1}{1+e^{-\alpha}}=\frac{e^{\alpha}}{1+e^{\alpha}}$
    \item The logit is equivalent to a linear function of the independent variables. The antilog of the logit function allows us to find the estimated regression equation
    \item $\text{logit}(p)=\ln(\frac{p}{1-p})=\beta_0+\beta_1x_1$ $\Rightarrow$ $\frac{p}{1-p}=e^{b_0+b_1 x_1}$ $\Rightarrow \dots \Rightarrow$ $\hat{p}=\frac{e^{b_0+b_1 x_1}}{1 + e^{b_0+b_1 x_1}}$, the estimated regression equation
\end{itemize}

\subsection{Estimating the Probability}

\begin{itemize}
    \item Logistic regression uses Maximum Likelihood Estimation (MLE) to estimate the parameters $\beta_0$ and $\beta_1$
    \item MLE seeks to maximize the likelihood function, which measures the probability of observing the given sample data
\end{itemize}

\subsubsection{Likelihood and Log-Likelihood}

\begin{itemize}
    \item The likelihood function $L(\beta_0, \beta_1 \mid y, x)$ is the joint probability of observing the given sample data as a function of the parameters $\beta_0$ and $\beta_1$
    \item For a binary outcome $y_i \in \{0,1\}$ with probability $p_i$ for $y_i=1$ and $(1-p_i)$ for $y_i=0$, the likelihood function for $n$ observations is:
    \[
    L(\beta_0, \beta_1) = \prod_{i=1}^{n} p_i^{y_i} (1-p_i)^{1-y_i}
    \]
    \item The log-likelihood function is the natural logarithm of the likelihood function:
    \[
    \ell(\beta_0, \beta_1) = \ln(L(\beta_0, \beta_1)) = \sum_{i=1}^{n} \left[ y_i \ln(p_i) + (1-y_i) \ln(1-p_i) \right]
    \]
    \item The log-likelihood function is easier to maximize because it converts the product of probabilities into a sum, which is mathematically simpler to handle
\end{itemize}

\subsubsection{Maximum Likelihood Estimation (MLE)}

\begin{itemize}
    \item MLE involves finding the parameter values $\beta_0$ and $\beta_1$ that maximize the log-likelihood function
    \item This is often done using iterative numerical optimization techniques, such as the Newton-Raphson method or gradient descent. There is no direct method like with gaussian (normal) models
    \item The estimated parameters $\hat{\beta}_0$ and $\hat{\beta}_1$ maximize the likelihood of observing the sample data, providing the best fit for the logistic regression model
\end{itemize}

\subsection{Multiple Logistic Regression}

\begin{itemize}
    \item Logistic regression can be extended to handle multiple predictors to model the probability of a binary outcome.
    \item The logistic regression model for multiple predictors takes the form:
    \[
    \log \left(\frac{p(X)}{1-p(X)}\right) = \beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p
    \]
    \item This can be rewritten to give the estimated probability:
    \[
    p(X) = \frac{e^{\beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p}}{1 + e^{\beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p}}
    \]
    \item Each coefficient $\beta_i$ represents the change in the log-odds of the response per unit increase in the corresponding predictor $X_i$, holding all other predictors constant.
\end{itemize}

\subsection{Coefficients and Interpretation of Results}

\begin{itemize}
    \item The coefficients in a logistic regression model indicate the effect of each predictor on the log-odds of the outcome, holding other variables constant.
    \item A positive coefficient increases the odds of the event, while a negative coefficient decreases the odds.
    \item However, when predictors are correlated (confounding), their individual effects may appear different when considered alone versus together, potentially leading to misleading interpretations.
    \item Proper interpretation requires understanding both the individual and combined effects of predictors on the response.
\end{itemize}

\subsubsection{Making Predictions}

\begin{itemize}
    \item Using the logistic regression equation, we can predict the probability of an event occurring for a given set of predictor values.
    \item For example, for a given set of predictor values $X = (X_1, \ldots, X_p)$, the predicted probability of the event is:
    \[
    \hat{p}(X) = \frac{e^{\beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p}}{1 + e^{\beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p}}
    \]
\end{itemize}

\subsection{Multinomial Logistic Regression}

\begin{itemize}
    \item Multinomial logistic regression extends binary logistic regression to classify a response variable with more than two classes.
    \item For $K$ classes, we select one as the baseline (e.g., class $K$), and model the probabilities for the other classes as:
    \[
    \operatorname{Pr}(Y=k \mid X=x)=\frac{e^{\beta_{k0}+\beta_{k1} x_1 + \cdots + \beta_{kp} x_p}}{1 + \sum_{l=1}^{K-1} e^{\beta_{l0} + \beta_{l1} x_1 + \cdots + \beta_{lp} x_p}}
    \]
    for $k = 1, \ldots, K-1$, with the baseline class:
    \[
    \operatorname{Pr}(Y=K \mid X=x)=\frac{1}{1 + \sum_{l=1}^{K-1} e^{\beta_{l0} + \beta_{l1} x_1 + \cdots + \beta_{lp} x_p}}
    \]
    \item The log odds for class $k$ versus the baseline $K$ is:
    \[
    \log \left(\frac{\operatorname{Pr}(Y=k \mid X=x)}{\operatorname{Pr}(Y=K \mid X=x)}\right) = \beta_{k0} + \beta_{k1} x_1 + \cdots + \beta_{kp} x_p
    \]
    \item Coefficients are interpreted in terms of log odds ratios relative to the baseline class. A one-unit increase in $X_j$ leads to a $\beta_{kj}$ increase in the log odds of class $k$ versus the baseline.
\end{itemize}

\subsubsection{Softmax Coding}

\begin{itemize}
    \item In softmax coding, no baseline class is chosen, and probabilities for all $K$ classes are modeled symmetrically:
    \[
    \operatorname{Pr}(Y=k \mid X=x)=\frac{e^{\beta_{k0}+\beta_{k1} x_1 + \cdots + \beta_{kp} x_p}}{\sum_{l=1}^{K} e^{\beta_{l0} + \beta_{l1} x_1 + \cdots + \beta_{lp} x_p}}
    \]
    \item The log odds ratio between two classes $k$ and $k^{\prime}$ is:
    \[
    \log \left(\frac{\operatorname{Pr}(Y=k \mid X=x)}{\operatorname{Pr}(Y=k^{\prime} \mid X=x)}\right) = (\beta_{k0} - \beta_{k^{\prime}0}) + \sum_{j=1}^{p} (\beta_{kj} - \beta_{k^{\prime}j}) x_j
    \]
\end{itemize}

\section{Bayes Classifiers, K-nearest neighbours}

\begin{itemize}
  \item For classifying data points, we can use Bayes' Theorem:
  \[
  p(\mathcal{C}_k \mid X) = \frac{p(X \mid \mathcal{C}_k) p(\mathcal{C}_k)}{p(X)}
  \]
  \item We can assign a data point to the class with the highest posterior probability:
  \[
  d(x) = \arg\max_k \, p(\mathcal{C}_k \mid X)
  \]
  \item Decision regions $\mathcal{R}_k$ are separated by boundaries based on $p(\mathcal{C}_k \mid X)$.
\end{itemize}

\subsection{Loss and Loss Matrix}

\begin{itemize}
  \item The loss matrix $L_{kj}$ captures the cost of predicting class $j$ when the true class is $k$. Each element of the matrix represents how costly a mistake is. For example, a large $L_{kj}$ indicates that predicting $j$ when the true class is $k$ is a serious error.
  \item In the matrix, rows represent the true class, and columns represent the predicted class:
  \[
  L_{kj} \quad \text{(Rows: True Class, Columns: Predicted Class)}
  \]
  \item The goal is to minimize the expected loss:
  \[
  \mathbb{E}[L] = \sum_{k} \sum_{j} \int_{\mathcal{R}_j} L_{kj} p(X) p(\mathcal{C}_k \mid X) \, dX
  \]
  This formula sums over all classes $k$ and predictions $j$, calculating the total cost of assigning a point $X$ to class $j$ while the true class is $k$. The integral over region $\mathcal{R}_j$ considers all feature space where class $j$ is predicted.
  \item To minimize loss, we assign $X$ to the class $j$ that minimizes:
  \[
  \sum_{k} L_{kj} p(\mathcal{C}_k \mid X)
  \]
  Here, for each possible predicted class $j$, we sum the cost $L_{kj}$ weighted by the posterior probability $p(\mathcal{C}_k \mid X)$, selecting the class $j$ that minimizes this sum. This ensures the decision balances the cost of errors with the likelihood of each class.
\end{itemize}

\subsection{Bayes Classifier}

\begin{itemize}
  \item The Bayes classifier is a specific case of minimizing expected loss, using a 0-1 loss function, defined as:
  \[
  L(j, k) = \begin{cases} 1, & j \neq k \\ 0, & j = k \end{cases}
  \]
  This is a loss function, where all errors are equal, unlike in the loss matrix with varying costs.
  
  \item The Bayes classifier assigns a new data point $X$ to the class with the highest posterior probability:
  \[
  d(x) = \arg\max_y \, p(Y = y \mid X)
  \]
\end{itemize}

\subsection{KNN for Classification}

\begin{itemize}
  \item Find the $K$ nearest neighbors to $X_0$ and estimate class probabilities based on neighbors' classes:
  \[
  p(Y = j \mid X_0) = \frac{1}{K} \sum_{i \in \mathcal{N}_0} I(y_i = j)
  \]
  \item Assign to the class with the highest posterior probability.
\end{itemize}

\subsubsection{Choosing $K$}

\begin{itemize}
  \item Low $K$: highly flexible boundaries.
  \item High $K$: less flexible, possibly too rigid.
  \item Choose $K$ via cross-validation to optimize performance.
\end{itemize}

\section{Linear Discriminant Analysis (LDA) and Quadratic Discriminant Analysis (QDA)}

\textbf{Oversimplified TLDR:}
\begin{itemize}
    \item Both assume all classes have a Gaussian distribution
    \item \textbf{LDA:}
    \begin{itemize}
        \item Assumes equal variation (same covariance) between classes
        \item Has straight (linear) decision boundaries
        \item Works well with less data due to fewer parameters to estimate
    \end{itemize}
    \item \textbf{QDA:}
    \begin{itemize}
        \item Assumes different variation (different covariance) between classes
        \item Has curved (quadratic) decision boundaries
        \item More flexible but requires more data to avoid overfitting
    \end{itemize}
\end{itemize}

\subsection{LDA}

\begin{itemize}
  \item LDA is a generative classification model that assumes the feature vectors $\mathbf{x} \in \mathbb{R}^p$ are generated from class-specific Gaussian distributions.
  \item Key assumptions of LDA:
  \begin{itemize}
    \item The class conditional densities $p(\mathbf{x} \mid \mathcal{C}_k)$ are multivariate Gaussian distributions.
    \item All classes share a common covariance matrix $\boldsymbol{\Sigma}$.
  \end{itemize}
  \item The class conditional density function is:
  \[
  p(\mathbf{x} \mid \mathcal{C}_k) = \frac{1}{(2\pi)^{p/2} |\boldsymbol{\Sigma}|^{1/2}} \exp\left( -\frac{1}{2} (\mathbf{x} - \boldsymbol{\mu}_k)^\top \boldsymbol{\Sigma}^{-1} (\mathbf{x} - \boldsymbol{\mu}_k) \right)
  \]
  where:
  \begin{itemize}
    \item $\boldsymbol{\mu}_k$ is the mean vector of class $\mathcal{C}_k$.
    \item $|\boldsymbol{\Sigma}|$ denotes the determinant of the covariance matrix.
    \item $p$ is the number of features.
  \end{itemize}
  \item Using Bayes' theorem, the posterior probability is:
  \[
  P(\mathcal{C}_k \mid \mathbf{x}) = \frac{\pi_k \, p(\mathbf{x} \mid \mathcal{C}_k)}{p(\mathbf{x})}
  \]
  where $\pi_k = P(\mathcal{C}_k)$ is the prior probability of class $\mathcal{C}_k$.
  \item The discriminant function for LDA is linear in $\mathbf{x}$:
  \[
  \delta_k(\mathbf{x}) = \mathbf{x}^\top \boldsymbol{\Sigma}^{-1} \boldsymbol{\mu}_k - \frac{1}{2} \boldsymbol{\mu}_k^\top \boldsymbol{\Sigma}^{-1} \boldsymbol{\mu}_k + \ln \pi_k
  \]
  \item Classification rule:
  \[
  \text{Assign } \mathbf{x} \text{ to class } \mathcal{C}_k \text{ if } \delta_k(\mathbf{x}) = \max_{j} \delta_j(\mathbf{x})
  \]
  \item Decision boundaries are linear hyperplanes separating the classes.
\end{itemize}

\subsubsection{Estimation of Parameters}

\begin{itemize}
  \item \textbf{Class Means}:
  \[
  \hat{\boldsymbol{\mu}}_k = \frac{1}{n_k} \sum_{i \in \mathcal{C}_k} \mathbf{x}_i
  \]
  where $n_k$ is the number of observations in class $\mathcal{C}_k$.
  \item \textbf{Common Covariance Matrix}:
  \[
  \hat{\boldsymbol{\Sigma}} = \frac{1}{n - K} \sum_{k=1}^K \sum_{i \in \mathcal{C}_k} (\mathbf{x}_i - \hat{\boldsymbol{\mu}}_k)(\mathbf{x}_i - \hat{\boldsymbol{\mu}}_k)^\top
  \]
  where $n$ is the total number of observations and $K$ is the number of classes.
  \item \textbf{Class Priors}:
  \[
  \hat{\pi}_k = \frac{n_k}{n}
  \]
  These can also be set based on prior knowledge.
\end{itemize}

\subsection{Quadratic Discriminant Analysis (QDA)}

\begin{itemize}
  \item QDA relaxes the assumption of a common covariance matrix, allowing each class to have its own covariance matrix $\boldsymbol{\Sigma}_k$.
  \item The class conditional density function is:
  \[
  p(\mathbf{x} \mid \mathcal{C}_k) = \frac{1}{(2\pi)^{p/2} |\boldsymbol{\Sigma}_k|^{1/2}} \exp\left( -\frac{1}{2} (\mathbf{x} - \boldsymbol{\mu}_k)^\top \boldsymbol{\Sigma}_k^{-1} (\mathbf{x} - \boldsymbol{\mu}_k) \right)
  \]
  \item The discriminant function for QDA is quadratic in $\mathbf{x}$:
  \[
  \delta_k(\mathbf{x}) = -\frac{1}{2} \ln |\boldsymbol{\Sigma}_k| - \frac{1}{2} (\mathbf{x} - \boldsymbol{\mu}_k)^\top \boldsymbol{\Sigma}_k^{-1} (\mathbf{x} - \boldsymbol{\mu}_k) + \ln \pi_k
  \]
  \item Classification rule:
  \[
  \text{Assign } \mathbf{x} \text{ to class } \mathcal{C}_k \text{ if } \delta_k(\mathbf{x}) = \max_{j} \delta_j(\mathbf{x})
  \]
  \item Decision boundaries are quadratic surfaces in the feature space.
\end{itemize}

\subsubsection{Estimation of Parameters}

\begin{itemize}
  \item \textbf{Class Means}:
  \[
  \hat{\boldsymbol{\mu}}_k = \frac{1}{n_k} \sum_{i \in \mathcal{C}_k} \mathbf{x}_i
  \]
  \item \textbf{Class Covariance Matrices}:
  \[
  \hat{\boldsymbol{\Sigma}}_k = \frac{1}{n_k - 1} \sum_{i \in \mathcal{C}_k} (\mathbf{x}_i - \hat{\boldsymbol{\mu}}_k)(\mathbf{x}_i - \hat{\boldsymbol{\mu}}_k)^\top
  \]
  \item \textbf{Class Priors}:
  \[
  \hat{\pi}_k = \frac{n_k}{n}
  \]
\end{itemize}

\subsection{Comparison between LDA and QDA}

\begin{itemize}
  \item \textbf{Assumptions}:
  \begin{itemize}
    \item LDA assumes equal covariance matrices across classes.
    \item QDA allows for different covariance matrices for each class.
  \end{itemize}
  \item \textbf{Decision Boundaries}:
  \begin{itemize}
    \item LDA produces linear decision boundaries.
    \item QDA produces quadratic decision boundaries.
  \end{itemize}
  \item \textbf{Complexity}:
  \begin{itemize}
    \item LDA estimates fewer parameters, which can be advantageous with limited data.
    \item QDA requires estimation of separate covariance matrices, increasing the number of parameters.
  \end{itemize}
  \item \textbf{Bias-Variance Tradeoff}:
  \begin{itemize}
    \item LDA may have higher bias if the assumption of equal covariances is violated.
    \item QDA may have higher variance due to more parameters, potentially leading to overfitting.
  \end{itemize}
  \item \textbf{When to Use}:
  \begin{itemize}
    \item LDA is preferred when class covariances are similar and the sample size is small.
    \item QDA is suitable when class covariances differ significantly and there is sufficient data.
  \end{itemize}
\end{itemize}

\subsection{Bias-Variance Tradeoff}

\begin{itemize}
  \item \textbf{LDA}:
  \begin{itemize}
    \item Lower variance due to fewer parameters.
    \item Potentially higher bias if covariance matrices are not equal.
  \end{itemize}
  \item \textbf{QDA}:
  \begin{itemize}
    \item Lower bias as it models each class more flexibly.
    \item Higher variance due to more parameters to estimate.
  \end{itemize}
  \item The choice depends on the dataset size and whether the equal covariance assumption holds.
\end{itemize}

\subsection{Decision Rule Interpretation}

\begin{itemize}
  \item The discriminant functions $\delta_k(\mathbf{x})$ can be seen as scoring functions.
  \item The classification rule selects the class with the highest score.
  \item The decision boundaries are determined by where the discriminant functions are equal.
\end{itemize}

\subsection{Practical Considerations}

\begin{itemize}
  \item \textbf{Regularization}:
  \begin{itemize}
    \item In cases of small sample sizes, covariance estimates may be unstable.
    \item Regularization techniques can be applied to covariance matrices to improve estimates.
  \end{itemize}
  \item \textbf{Computational Efficiency}:
  \begin{itemize}
    \item LDA is computationally less intensive due to a single covariance matrix.
    \item QDA requires inversion of multiple covariance matrices.
  \end{itemize}
  \item \textbf{Assumption Checking}:
  \begin{itemize}
    \item It's important to check the assumption of normality.
    \item Techniques like plotting or statistical tests can be used to assess distributional assumptions.
  \end{itemize}
\end{itemize}

\section{Multivariate Gaussian Distribution, LDA, and QDA with Multiple Features}

\textbf{Oversimplified TLDR:}
\begin{itemize}
    \item Multivariate Gaussian distributions extend the normal distribution to multiple dimensions.
    \item \textbf{LDA with Multiple Features:}
    \begin{itemize}
        \item Assumes class conditional densities are multivariate Gaussian with a common covariance matrix.
        \item Decision boundaries are linear hyperplanes.
    \end{itemize}
    \item \textbf{QDA with Multiple Features:}
    \begin{itemize}
        \item Assumes class conditional densities are multivariate Gaussian with class-specific covariance matrices.
        \item Decision boundaries are quadratic surfaces.
    \end{itemize}
\end{itemize}

\subsection{Multivariate Data}

\begin{itemize}
    \item \textbf{Random Vectors:} If $X_1, X_2, \ldots, X_p$ are real-valued random variables, we can define a $p$-dimensional random vector $\mathbf{X}$ as:
    \[
    \mathbf{X} = \begin{bmatrix} X_1 \\ \vdots \\ X_p \end{bmatrix}
    \]
    \item \textbf{Expectation (Mean):} The expectation of $\mathbf{X}$ is:
    \[
    \mathbb{E}[\mathbf{X}] = \begin{bmatrix} \mathbb{E}[X_1] \\ \vdots \\ \mathbb{E}[X_p] \end{bmatrix}
    \]
    \item \textbf{Sample Mean:} Given $n$ observations $\{\mathbf{X}_1, \mathbf{X}_2, \ldots, \mathbf{X}_n\}$, the sample mean is:
    \[
    \hat{\boldsymbol{\mu}} = \frac{1}{n} \sum_{i=1}^n \mathbf{X}_i
    \]
    \item \textbf{Variance and Covariance Matrix:} The variance of $\mathbf{X}$ is the covariance matrix $\boldsymbol{\Sigma}$, defined as:
    \[
    \operatorname{Var}[\mathbf{X}] = \mathbb{E}\left[(\mathbf{X} - \mathbb{E}[\mathbf{X}]) (\mathbf{X} - \mathbb{E}[\mathbf{X}])^\top\right]
    \]
    where $\boldsymbol{\Sigma}$ is a $p \times p$ matrix with entries:
    \[
    \Sigma_{ij} = \operatorname{Cov}[X_i, X_j]
    \]
\end{itemize}

\subsection{The Multivariate Normal Distribution}

\begin{itemize}
    \item The $p$-dimensional \textbf{Multivariate Normal Distribution} with mean vector $\boldsymbol{\mu}$ and covariance matrix $\boldsymbol{\Sigma}$ has the probability density function:
    \[
    p(\mathbf{x}; \boldsymbol{\mu}, \boldsymbol{\Sigma}) = \frac{1}{(2\pi)^{p/2} |\boldsymbol{\Sigma}|^{1/2}} \exp\left\{ -\frac{1}{2} (\mathbf{x} - \boldsymbol{\mu})^\top \boldsymbol{\Sigma}^{-1} (\mathbf{x} - \boldsymbol{\mu}) \right\}
    \]
    \item The exponent involves the \textbf{Mahalanobis distance}:
    \[
    (\mathbf{x} - \boldsymbol{\mu})^\top \boldsymbol{\Sigma}^{-1} (\mathbf{x} - \boldsymbol{\mu}) = \|\mathbf{x} - \boldsymbol{\mu}\|_{\boldsymbol{\Sigma}}^2
    \]
    \item \textbf{Special Case:} When $\boldsymbol{\Sigma} = \mathbf{I}$ (identity matrix), the variables are independent standard normal, and the Mahalanobis distance reduces to the Euclidean distance:
    \[
    \|\mathbf{x} - \boldsymbol{\mu}\|_{\mathbf{I}}^2 = \sum_{i=1}^p (x_i - \mu_i)^2
    \]
    \item \textbf{Properties:}
    \begin{itemize}
        \item Any linear transformation or translation of a multivariate normal variable is also multivariate normal.
        \item Marginal distributions of subsets of variables are multivariate normal.
    \end{itemize}
\end{itemize}

\subsection{Linear Discriminant Analysis (LDA) with Multiple Features}

\begin{itemize}
    \item \textbf{Assumptions:}
    \begin{itemize}
        \item Each class conditional density $p(\mathbf{x} \mid \mathcal{C}_k)$ is a multivariate normal distribution with mean $\boldsymbol{\mu}_k$ and common covariance matrix $\boldsymbol{\Sigma}$:
        \[
        p(\mathbf{x} \mid \mathcal{C}_k) = \frac{1}{(2\pi)^{p/2} |\boldsymbol{\Sigma}|^{1/2}} \exp\left\{ -\frac{1}{2} (\mathbf{x} - \boldsymbol{\mu}_k)^\top \boldsymbol{\Sigma}^{-1} (\mathbf{x} - \boldsymbol{\mu}_k) \right\}
        \]
    \end{itemize}
    \item \textbf{Discriminant Function:}
    \begin{itemize}
        \item The discriminant function for class $\mathcal{C}_k$ is derived from the log of the joint probability:
        \[
        g_k(\mathbf{x}) = \ln \pi_k - \frac{1}{2} (\mathbf{x} - \boldsymbol{\mu}_k)^\top \boldsymbol{\Sigma}^{-1} (\mathbf{x} - \boldsymbol{\mu}_k)
        \]
        \item Simplifies to:
        \[
        g_k(\mathbf{x}) = \mathbf{x}^\top \boldsymbol{\Sigma}^{-1} \boldsymbol{\mu}_k - \frac{1}{2} \boldsymbol{\mu}_k^\top \boldsymbol{\Sigma}^{-1} \boldsymbol{\mu}_k + \ln \pi_k
        \]
    \end{itemize}
    \item \textbf{Classification Rule:}
    \begin{itemize}
        \item Assign $\mathbf{x}$ to the class with the highest discriminant score:
        \[
        \text{Classify } \mathbf{x} \text{ to class } \mathcal{C}_k \text{ if } g_k(\mathbf{x}) = \max_j g_j(\mathbf{x})
        \]
    \end{itemize}
    \item \textbf{Decision Boundaries:}
    \begin{itemize}
        \item The boundaries between classes are linear hyperplanes where $g_j(\mathbf{x}) = g_k(\mathbf{x})$.
    \end{itemize}
\end{itemize}

\subsection{Quadratic Discriminant Analysis (QDA) with Multiple Features}

\begin{itemize}
    \item \textbf{Assumptions:}
    \begin{itemize}
        \item Each class conditional density $p(\mathbf{x} \mid \mathcal{C}_k)$ is a multivariate normal distribution with mean $\boldsymbol{\mu}_k$ and covariance matrix $\boldsymbol{\Sigma}_k$:
        \[
        p(\mathbf{x} \mid \mathcal{C}_k) = \frac{1}{(2\pi)^{p/2} |\boldsymbol{\Sigma}_k|^{1/2}} \exp\left\{ -\frac{1}{2} (\mathbf{x} - \boldsymbol{\mu}_k)^\top \boldsymbol{\Sigma}_k^{-1} (\mathbf{x} - \boldsymbol{\mu}_k) \right\}
        \]
    \end{itemize}
    \item \textbf{Discriminant Function:}
    \begin{itemize}
        \item The discriminant function for class $\mathcal{C}_k$ is:
        \[
        g_k(\mathbf{x}) = \ln \pi_k - \frac{1}{2} \ln |\boldsymbol{\Sigma}_k| - \frac{1}{2} (\mathbf{x} - \boldsymbol{\mu}_k)^\top \boldsymbol{\Sigma}_k^{-1} (\mathbf{x} - \boldsymbol{\mu}_k)
        \]
    \end{itemize}
    \item \textbf{Classification Rule:}
    \begin{itemize}
        \item Assign $\mathbf{x}$ to the class with the highest $g_k(\mathbf{x})$.
    \end{itemize}
    \item \textbf{Decision Boundaries:}
    \begin{itemize}
        \item The boundaries between classes are quadratic surfaces defined by $g_j(\mathbf{x}) = g_k(\mathbf{x})$.
    \end{itemize}
\end{itemize}

\subsection{Constructing Decision Boundaries}

\begin{itemize}
    \item \textbf{LDA Decision Boundary:}
    \begin{itemize}
        \item Between classes $\mathcal{C}_j$ and $\mathcal{C}_k$, the decision boundary is where:
        \[
        g_j(\mathbf{x}) = g_k(\mathbf{x})
        \]
        \item Simplifies to a linear equation in $\mathbf{x}$.
    \end{itemize}
    \item \textbf{Effect of Class Priors:}
    \begin{itemize}
        \item Increasing the prior probability $\pi_k$ shifts the decision boundary away from $\boldsymbol{\mu}_k$.
        \item The term $\ln \pi_k$ influences the discriminant function.
    \end{itemize}
    \item \textbf{QDA Decision Boundary:}
    \begin{itemize}
        \item Boundaries are more complex due to class-specific covariance matrices.
        \item The equation $g_j(\mathbf{x}) = g_k(\mathbf{x})$ results in quadratic terms.
    \end{itemize}
\end{itemize}

\subsection{Issues with LDA and QDA}

\begin{itemize}
    \item \textbf{Number of Parameters:}
    \begin{itemize}
        \item Covariance matrix $\boldsymbol{\Sigma}$ has $\frac{p(p+1)}{2}$ parameters.
        \item LDA estimates one covariance matrix; QDA estimates one per class.
    \end{itemize}
    \item \textbf{Bias-Variance Trade-Off:}
    \begin{itemize}
        \item LDA has lower variance but higher bias if the assumption of equal covariances is incorrect.
        \item QDA has lower bias but higher variance due to more parameters.
    \end{itemize}
    \item \textbf{Naive Bayes Classifier:}
    \begin{itemize}
        \item Assumes independence among features, leading to diagonal covariance matrices.
        \item Simplifies computation but may not capture feature correlations.
    \end{itemize}
\end{itemize}

\subsection{Case Studies}

\subsubsection{Case 1: Equal Covariance Matrices (No Correlations)}

\begin{itemize}
    \item Covariance matrices are equal and features are uncorrelated.
    \item LDA performs well due to matching assumptions.
    \item Logistic regression and Naive Bayes provide similar performance.
\end{itemize}

\subsubsection{Case 2: Unequal Covariance Matrices (No Correlations)}

\begin{itemize}
    \item Covariance matrices differ but features remain uncorrelated.
    \item QDA outperforms LDA as it models class-specific covariances.
    \item Logistic regression may require additional features (e.g., interaction terms) to capture the non-linearity.
\end{itemize}

\subsubsection{Case 3: Unequal Covariance Matrices (With Correlations)}

\begin{itemize}
    \item Covariance matrices differ and features are correlated.
    \item QDA and logistic regression with interaction terms perform best.
    \item Naive Bayes performs poorly due to the independence assumption.
\end{itemize}

\subsection{Test Error Comparison}

\begin{center}
\begin{tabular}{lccccc}
\hline
 & LDA & QDA & Naive Bayes & Logistic Regression & Logistic Regression (Quadratic) \\
\hline
Case 1 & 1.20\% & 1.30\% & 1.33\% & 1.40\% & 1.33\% \\
Case 2 & 8.00\% & 7.27\% & 7.43\% & 7.60\% & 7.57\% \\
Case 3 & 2.27\% & 0.63\% & 2.20\% & 0.87\% & 0.60\% \\
\hline
\end{tabular}
\end{center}

\subsection{LDA vs Logistic Regression}

\begin{itemize}
    \item \textbf{Relationship:}
    \begin{itemize}
        \item Both produce linear decision boundaries under certain conditions.
        \item LDA assumes normality and equal covariances; logistic regression does not.
    \end{itemize}
    \item \textbf{Log-Posterior Odds:}
    \begin{itemize}
        \item In logistic regression, the log-odds are modeled directly:
        \[
        \log\left( \frac{P(\mathcal{C}_k \mid \mathbf{x})}{P(\mathcal{C}_K \mid \mathbf{x})} \right) = \boldsymbol{\beta}_k^\top \mathbf{x} + \beta_{0k}
        \]
        \item LDA implies a linear model for log-odds when the normality assumptions hold.
    \end{itemize}
    \item \textbf{Advantages of LDA:}
    \begin{itemize}
        \item More efficient if the normality and equal covariance assumptions are valid.
        \item Naturally incorporates class priors.
    \end{itemize}
    \item \textbf{Advantages of Logistic Regression:}
    \begin{itemize}
        \item Flexible in including various types of predictors and transformations.
        \item Does not rely on distributional assumptions.
    \end{itemize}
\end{itemize}

\subsection{Practical Considerations}

\begin{itemize}
    \item \textbf{Regularization:}
    \begin{itemize}
        \item In high dimensions, covariance estimates may be unstable.
        \item Regularization techniques can improve estimates (e.g., shrinkage methods).
    \end{itemize}
    \item \textbf{Computational Efficiency:}
    \begin{itemize}
        \item LDA is computationally less intensive than QDA.
        \item QDA requires inversion of multiple covariance matrices.
    \end{itemize}
    \item \textbf{Assumption Checking:}
    \begin{itemize}
        \item Check normality of features using plots or tests.
        \item Verify the homogeneity of covariances for LDA.
    \end{itemize}
    \item \textbf{Feature Engineering:}
    \begin{itemize}
        \item Including interaction terms or polynomial features can improve logistic regression.
        \item Dimensionality reduction techniques (e.g., PCA) may be beneficial.
    \end{itemize}
\end{itemize}

\end{document}